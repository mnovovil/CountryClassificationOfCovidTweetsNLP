{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition ##  \n",
    "Lisa Pink and Miguel Novo Villar - DSCC465: Introduction to Statistical Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text. \n",
    "# Source: https://www.kaggle.com/code/clmentbisaillon/twitter-customer-support-data-cleaning\n",
    "rare = re.compile(r\"\\^\\S*\")\n",
    "new_line = re.compile(r\"\\n+\\S*\")\n",
    "sig = re.compile(r\"-\\S*\")\n",
    "\n",
    "#Initial preprocessing function\n",
    "def preprocessor(data):\n",
    "    corpus = []\n",
    "    for i in range(len(data)):\n",
    "        #remove urls\n",
    "        tweet= re.sub(r'http\\S+', ' ', data[\"full_text\"][i][2:-1]) #[2:-1] removes the b' caracters\n",
    "\n",
    "        #remove mentions\n",
    "        tweet = re.sub('@[A-Za-z0–9]+', '', tweet)\n",
    "        tweet = re.sub(\"@[\\w]*\",\"\",tweet)\n",
    "        \n",
    "        # # Contractions (Source: https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert/notebook)\n",
    "        tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "        tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "        tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "        tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "        tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "        tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "        tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "        tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "        tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "        tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "        tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "        tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "        tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "        tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "        tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "        tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "        tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "        tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "        tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "        tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "        tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "        tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "        tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "        tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "        tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "        tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "        tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "        tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "        tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "        tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "        tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "        tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "        tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "        tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "        tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "        tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "        tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "        tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "        tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "        tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "        tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "        tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "        tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "        tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "        tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "        tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "        tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "        tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "        tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "        tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "        tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "        tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "        tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "        tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "        tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "        tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "        tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "        tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "        tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "        tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "        tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "        tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "        tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "        tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "        tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "        tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "        tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "        tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "        tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "        tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "        tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "        tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "        tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "        tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "        tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "        tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "        tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "        tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "        tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "        tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "        tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "        tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "        tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "        tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "        tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "        tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "        tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "        tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "        tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "        tweet = re.sub(r\"donå«t\", \"do not\", tweet) \n",
    "        \n",
    "        # Character entity references\n",
    "        tweet = re.sub(r\"&gt;\", \"\", tweet)\n",
    "        tweet = re.sub(r\"&lt;\", \"\", tweet)\n",
    "        tweet = re.sub(r\"&amp;\", \"\", tweet)\n",
    "\n",
    "        # Typos, slang and informal abbreviations\n",
    "        tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "        tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "        tweet = re.sub(r\"USAgov\", \"USA government\", tweet)        \n",
    "\n",
    "        #remove rt\n",
    "        tweet = re.sub(\"RT @[\\w]*:\",\"\",tweet)#removing rt\n",
    "        tweet = re.sub('RT[\\s]+', '', tweet) # Removing RT\n",
    "        \n",
    "        #remove emoji\n",
    "        tweet=re.sub(\"[^\\w\\s#@/:%.,_-]\", \"\", tweet, flags=re.UNICODE)#remove emoji\n",
    "        \n",
    "        tweet = tweet.replace('x', '')\n",
    "        \n",
    "        #remove html tags\n",
    "        tweet = re.sub(r'<.*?>',' ', tweet) \n",
    "\n",
    "        #signatures\n",
    "        tweet = sig.sub(r'', tweet)\n",
    "\n",
    "        #rare\n",
    "        tweet = rare.sub(r'', tweet)\n",
    "\n",
    "        #new line\n",
    "        tweet = new_line.sub(r'.', tweet)\n",
    "\n",
    "        #remove digits\n",
    "        tweet = re.sub(r'\\d+',' ', tweet)\n",
    "        \n",
    "        # #remove hashtags\n",
    "        # tweet = re.sub(r'#\\w+',' ', tweet)\n",
    "        \n",
    "        #remove white\n",
    "        tweet = re.sub(\"^\\\\s+|\\\\s+$\", \"\", tweet)  # Remove leading and trailing white space\n",
    "        #unite multispace\n",
    "        tweet = ' '.join(tweet.split())\n",
    "        review = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus   \n",
    "\n",
    "#FUNCTIONS APPLIED\n",
    "\n",
    "def convert_list_of_str_to_list_lists(X):\n",
    "    return [list(sentence) for sentence in X]\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend([\"amp\",\"wa\",\"ta\",\"ha\",\"nn\",\"ie\",\"ste\"])\n",
    "    return [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "def tokenize_words(X, tweet_tokenizer=True, rmv_stopwords=True):\n",
    "    if tweet_tokenizer:\n",
    "        tokenize = TweetTokenizer().tokenize\n",
    "    else:\n",
    "        tokenize = nltk.word_tokenize\n",
    "\n",
    "    word_tokens = [tokenize(sentence) for sentence in X]\n",
    "\n",
    "    if rmv_stopwords:\n",
    "        return [remove_stopwords(tokens) for tokens in word_tokens]\n",
    "    else:\n",
    "        return word_tokens\n",
    "\n",
    "def pos_tagging_words(X):\n",
    "    return [[nltk.pos_tag(sentence)] for sentence in X]\n",
    "\n",
    "def lemmatize_words(X, lemmatizer):\n",
    "    X_lemmatize = list()\n",
    "    for sentence in X:\n",
    "        X_lemmatize.append([[lemmatizer.lemmatize(word)] for word in sentence])\n",
    "    return X_lemmatize\n",
    "\n",
    "def prepare_tokens_for_vectorize(X):\n",
    "    X_preproc = list()\n",
    "    for sentence_tok in X:\n",
    "        tokens = [tok[0] for tok in sentence_tok]\n",
    "        X_preproc.append(\" \".join(tokens))\n",
    "\n",
    "    return X_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp/ipykernel_80872/2282867806.py:1: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_train = pd.read_csv(\"training_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"training_data.csv\")\n",
    "data_test = pd.read_csv(\"test_data(1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember the #WuhanCoronaVirus? The pandemic w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>WuhanCoronaVirus KillerCuomo</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My sources @WhiteHouse say 2 tactics will be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Trump</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll venture a wild guess: If you were running...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Pakistan (#GreenStimulus = #Nature protection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Pakistan GreenStimulus Nature Green</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🇺🇸 Pandémie de #coronavirus: 30 pasteurs améri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>coronavirus COVID__19 COVIDー19</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239995</th>\n",
       "      <td>Aa Likes, Retweets yentra 🙏\\n🔥🔥🔥\\n#Mastеr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Mastеr</td>\n",
       "      <td>new_zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239996</th>\n",
       "      <td>Very interesting\\nAny thoughts?\\n\\n#TheFive #T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TheFive Trump2020 KAG2020 mondaythoughts COVID...</td>\n",
       "      <td>new_zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239997</th>\n",
       "      <td>As we deal with #COVID19 don't forget that #Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>COVID19 Christians persecution Nigeria</td>\n",
       "      <td>new_zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239998</th>\n",
       "      <td>While we hit 150,000 in #COVID19 deaths, the P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>COVID19</td>\n",
       "      <td>new_zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239999</th>\n",
       "      <td>This too shall pass #Covid_19 . May remain sta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Covid_19 HopeAlive</td>\n",
       "      <td>new_zealand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Remember the #WuhanCoronaVirus? The pandemic w...   \n",
       "1       My sources @WhiteHouse say 2 tactics will be u...   \n",
       "2       I'll venture a wild guess: If you were running...   \n",
       "3       #Pakistan (#GreenStimulus = #Nature protection...   \n",
       "4       🇺🇸 Pandémie de #coronavirus: 30 pasteurs améri...   \n",
       "...                                                   ...   \n",
       "239995          Aa Likes, Retweets yentra 🙏\\n🔥🔥🔥\\n#Mastеr   \n",
       "239996  Very interesting\\nAny thoughts?\\n\\n#TheFive #T...   \n",
       "239997  As we deal with #COVID19 don't forget that #Ch...   \n",
       "239998  While we hit 150,000 in #COVID19 deaths, the P...   \n",
       "239999  This too shall pass #Covid_19 . May remain sta...   \n",
       "\n",
       "       reply_to_screen_name is_quote is_retweet  \\\n",
       "0                       NaN    False       True   \n",
       "1                       NaN    False       True   \n",
       "2                       NaN    False       True   \n",
       "3                       NaN    False       True   \n",
       "4                       NaN    False       True   \n",
       "...                     ...      ...        ...   \n",
       "239995                  NaN     TRUE       TRUE   \n",
       "239996                  NaN    FALSE       TRUE   \n",
       "239997                  NaN     TRUE       TRUE   \n",
       "239998                  NaN    FALSE       TRUE   \n",
       "239999                  NaN    FALSE       TRUE   \n",
       "\n",
       "                                                 hashtags      country  \n",
       "0                            WuhanCoronaVirus KillerCuomo           us  \n",
       "1                                                   Trump           us  \n",
       "2                                                 COVID19           us  \n",
       "3                     Pakistan GreenStimulus Nature Green           us  \n",
       "4                          coronavirus COVID__19 COVIDー19           us  \n",
       "...                                                   ...          ...  \n",
       "239995                                             Mastеr  new_zealand  \n",
       "239996  TheFive Trump2020 KAG2020 mondaythoughts COVID...  new_zealand  \n",
       "239997             COVID19 Christians persecution Nigeria  new_zealand  \n",
       "239998                                            COVID19  new_zealand  \n",
       "239999                                 Covid_19 HopeAlive  new_zealand  \n",
       "\n",
       "[240000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50cad90f2f273938ceabd83189106b19f0fe603f9ecc8bddf15d51c3f9654c30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
